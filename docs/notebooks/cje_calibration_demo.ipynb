{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CJE Calibration Demo\n",
    "\n",
    "This notebook demonstrates how to use CJE (Causal Judge Evaluation) to calibrate AI survey responses against human ground truth.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Get valid estimates using only 5-10% human labels\n",
    "- Proper uncertainty quantification with confidence intervals\n",
    "- Statistical comparison between models/policies\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install edsl[cje]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development (run from repo root): pip install -e \".[cje]\"\n",
    "# For production: pip install \"edsl[cje]\"\n",
    "\n",
    "# If running from docs/notebooks/, install the local package:\n",
    "%pip install -e ../.. --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify edsl is installed from the local repo\n",
    "import edsl\n",
    "print(f\"EDSL version: {edsl.__version__}\")\n",
    "print(f\"Location: {edsl.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run a Survey with Multiple Models\n",
    "\n",
    "First, we create a simple sentiment survey and run it with two different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsl import Survey, QuestionLinearScale, Model, Agent\n",
    "\n",
    "# Create a simple sentiment question\n",
    "q = QuestionLinearScale(\n",
    "    question_name=\"sentiment\",\n",
    "    question_text=\"Rate the sentiment of this text: '{{ text }}'\",\n",
    "    question_options=[1, 2, 3, 4, 5],\n",
    "    option_labels={1: \"Very Negative\", 5: \"Very Positive\"}\n",
    ")\n",
    "\n",
    "survey = Survey([q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsl import ScenarioList\n",
    "\n",
    "# Sample texts to analyze\n",
    "texts = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"This is okay, nothing special.\",\n",
    "    \"Terrible experience, would not recommend.\",\n",
    "    \"Pretty good overall, minor issues.\",\n",
    "    \"Absolutely fantastic, exceeded expectations!\",\n",
    "    \"Meh, it's fine I guess.\",\n",
    "    \"Worst purchase ever.\",\n",
    "    \"Great quality, fast shipping.\",\n",
    "    \"Not worth the money.\",\n",
    "    \"Highly recommend to everyone!\",\n",
    "]\n",
    "\n",
    "scenarios = ScenarioList.from_list(\"text\", texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with two models\n",
    "models = [Model(\"gpt-4o\"), Model(\"claude-sonnet-4-20250514\")]\n",
    "\n",
    "results = survey.by(scenarios).by(models).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "results.select(\"model\", \"text\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Human Labels (Oracle)\n",
    "\n",
    "In practice, you would collect human ratings for a sample of responses.\n",
    "Here we simulate this with \"ground truth\" ratings for ~50% of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Simulate human labels for some samples\n",
    "# In practice, you would collect these from actual human raters\n",
    "human_labels = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    # Add human label for ~50% of samples\n",
    "    if random.random() < 0.5:\n",
    "        # Simulate a human rating (1-5 scale)\n",
    "        text = result[\"scenario\"][\"text\"]\n",
    "        if \"love\" in text or \"amazing\" in text or \"fantastic\" in text:\n",
    "            human_labels.append(5)\n",
    "        elif \"terrible\" in text or \"worst\" in text:\n",
    "            human_labels.append(1)\n",
    "        elif \"good\" in text or \"great\" in text:\n",
    "            human_labels.append(4)\n",
    "        elif \"okay\" in text or \"fine\" in text or \"meh\" in text:\n",
    "            human_labels.append(3)\n",
    "        else:\n",
    "            human_labels.append(3)\n",
    "    else:\n",
    "        human_labels.append(None)  # No human label for this sample\n",
    "\n",
    "print(f\"Human labels collected: {sum(1 for x in human_labels if x is not None)}/{len(human_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calibrate with CJE\n",
    "\n",
    "Now we use CJE to calibrate the AI responses against human labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform 1-5 scale to 0-1 for CJE\n",
    "scale_transform = lambda x: (x - 1) / 4\n",
    "\n",
    "# Calibrate\n",
    "cal_result = results.calibrate(\n",
    "    question_name=\"sentiment\",\n",
    "    oracle_labels=human_labels,\n",
    "    policy_column=\"model\",\n",
    "    score_transform=scale_transform,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# View calibrated estimates and forest plot\nprint(cal_result)\ncal_result.plot();"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare Models\n",
    "\n",
    "Use statistical testing to compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model\n",
    "print(f\"Best model: {cal_result.best_policy()}\")\n",
    "print(f\"Ranking: {cal_result.ranking()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison between models\n",
    "policies = list(cal_result.estimates.keys())\n",
    "if len(policies) >= 2:\n",
    "    comparison = cal_result.compare(policies[0], policies[1])\n",
    "    print(f\"\\nComparison: {policies[0]} vs {policies[1]}\")\n",
    "    print(f\"Difference: {comparison.difference:.3f}\")\n",
    "    print(f\"p-value: {comparison.p_value:.3f}\")\n",
    "    print(f\"Significant at Î±=0.05: {comparison.significant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "CJE calibration provides:\n",
    "\n",
    "1. **Efficient use of human labels** - Only need 5-10% oracle labels to calibrate\n",
    "2. **Valid point estimates** - Corrects for systematic biases in AI judge scores\n",
    "3. **Uncertainty quantification** - Confidence intervals for each policy\n",
    "4. **Statistical comparisons** - Proper hypothesis testing between policies\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Collect real human labels for your survey responses\n",
    "- Use `CalibrationResult.compare()` to make data-driven model selection decisions\n",
    "- Monitor for drift by re-calibrating when models are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}